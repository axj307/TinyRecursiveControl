# Quick Start Guide

## âœ… Setup Complete!

Your TinyRecursiveControl project is fully set up and ready to use!

### What's Been Created

**Project Structure:**
```
TinyRecursiveControl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ encoders.py                    âœ“ State encoders
â”‚   â”‚   â”œâ”€â”€ decoders.py                    âœ“ Control decoders
â”‚   â”‚   â”œâ”€â”€ recursive_reasoning.py         âœ“ Recursive refinement
â”‚   â”‚   â””â”€â”€ tiny_recursive_control.py      âœ“ Main TRC model
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ lqr_generator.py               âœ“ LQR dataset generator
â”œâ”€â”€ TinyRecursiveModels/                   âœ“ Reference implementation
â”œâ”€â”€ test_model.py                          âœ“ Verification script
â”œâ”€â”€ README.md                              âœ“ Full documentation
â””â”€â”€ IMPLEMENTATION_GUIDE.md                âœ“ Detailed guide
```

**Environment:**
- âœ“ Conda environment `trm_control` created
- âœ“ Python 3.10 installed
- âœ“ PyTorch 2.9.0 + CUDA 12 installed
- âœ“ All dependencies installed

---

## Next Steps (Run These Commands)

###  1. Activate Environment

```bash
cd /orcd/home/002/amitjain/project/TinyRecursiveControl
conda activate trm_control
```

### 2. Test the Implementation

```bash
python test_model.py
```

Expected output: All 4 tests should PASS

### 3. Generate Training Data

```bash
# Generate 10,000 optimal LQR trajectories
python src/data/lqr_generator.py \
    --num_samples 10000 \
    --output_dir data/double_integrator_lqr \
    --num_steps 15 \
    --time_horizon 5.0
```

This will create:
- `data/double_integrator_lqr/lqr_dataset.pkl`
- `data/double_integrator_lqr/lqr_dataset.npz`

### 4. Try the Model Interactively

```python
import torch
import sys
sys.path.insert(0, 'src')

from models import TinyRecursiveControl

# Create model
model = TinyRecursiveControl.create_medium()  # ~3M parameters

# Define problem
current_state = torch.tensor([[0.0, 0.0]])    # [position, velocity]
target_state = torch.tensor([[1.0, 0.0]])     # Goal: pos=1, vel=0

# Generate controls
output = model(current_state, target_state)
controls = output['controls']

print(f"Control sequence shape: {controls.shape}")
print(f"Controls: {controls[0, :, 0]}")
```

---

## Key Differences vs Your LLM Approach

| Feature | Your LLM | TinyRecursiveControl |
|---------|----------|----------------------|
| Model Size | 3B params | 1-5M params (600x smaller) |
| Input | Text prompts | Numeric states |
| Output | Text â†’ parse | Direct controls |
| Memory | ~6 GB | ~20 MB |
| Speed | ~100ms | ~5ms |
| Training Data | Text + control | Optimal trajectories |

---

## Integration with Your Existing Code

### Use Your Dynamics

```python
# Import your existing dynamics
sys.path.append('/orcd/home/002/amitjain/project/Unsloth/Qwen/testRL/working_OG_origin/Control_GRPO_1/src')
from dynamics import propagateOneStep

def wrap_dynamics(dt=0.33):
    """Wrap your dynamics for TRC."""
    def dynamics_fn(state_batch, control_batch):
        # Simulate using your propagateOneStep
        # ... (see IMPLEMENTATION_GUIDE.md for full code)
        pass
    return dynamics_fn

# Use with TRC
dynamics = wrap_dynamics()
output = model(current, target, dynamics_fn=dynamics)
```

### Use Your Reward Function

```python
from reward import navigation_reward_func

# TRC generates controls
output = model(current_states, target_states)

# Format for your reward function
controls_text = format_for_reward(output['controls'])

# Evaluate
rewards = navigation_reward_func(
    completions=controls_text,
    initial_state=initial_states,
    target_state=target_states,
)
```

---

## Running Experiments

### Experiment 1: Baseline Performance

```bash
# 1. Generate data
python src/data/lqr_generator.py --num_samples 10000

# 2. Load and test
python -c "
import torch
import numpy as np
import sys
sys.path.insert(0, 'src')
from models import TinyRecursiveControl

# Load LQR data
data = np.load('data/double_integrator_lqr/lqr_dataset.npz')

# Create model
model = TinyRecursiveControl.create_medium()

# Test on first 10 samples
initial = torch.tensor(data['initial_states'][:10], dtype=torch.float32)
target = torch.tensor(data['target_states'][:10], dtype=torch.float32)

output = model(initial, target)
print(f'âœ“ Generated controls for {len(initial)} samples')
print(f'  Control shape: {output[\"controls\"].shape}')
"
```

### Experiment 2: Compare Model Sizes

See `test_model.py` - Test 3 compares small/medium/large models

### Experiment 3: Recursive Refinement Study

Modify `TRCConfig.num_outer_cycles` to test different iteration counts

---

## Troubleshooting

### Issue: Import errors

```bash
# Make sure you're in the right directory
cd /orcd/home/002/amitjain/project/TinyRecursiveControl

# And environment is activated
conda activate trm_control
```

### Issue: CUDA errors

```python
# Force CPU mode
model = TinyRecursiveControl.create_small()
current = torch.tensor([[0.0, 0.0]])
target = torch.tensor([[1.0, 0.0]])

# Ensure CPU
current = current.cpu()
target = target.cpu()

output = model(current, target)
```

### Issue: Need help

Check these files:
1. `README.md` - Full documentation
2. `IMPLEMENTATION_GUIDE.md` - Detailed integration guide
3. `test_model.py` - Working examples

---

## What to Do Next

1. **Verify everything works:**
   ```bash
   python test_model.py
   ```

2. **Generate training data:**
   ```bash
   python src/data/lqr_generator.py --num_samples 10000
   ```

3. **Compare with your LLM:**
   - Run same test cases on both
   - Measure: error, cost, time, memory
   - Document results

4. **Iterate on architecture:**
   - Try different `latent_dim`: 64, 128, 256
   - Experiment with `num_outer_cycles`: 3, 5, 7
   - Test residual vs full decoder

5. **Scale up:**
   - Increase training data (10K â†’ 50K)
   - Add supervised pretraining (coming soon)
   - Try RL fine-tuning with your reward function

---

## Files Overview

- **Core Implementation:**
  - `src/models/tiny_recursive_control.py` - Main model
  - `src/models/encoders.py` - State encoders
  - `src/models/decoders.py` - Control decoders
  - `src/models/recursive_reasoning.py` - Refinement logic

- **Data:**
  - `src/data/lqr_generator.py` - Generate optimal trajectories

- **Testing:**
  - `test_model.py` - Verify implementation

- **Documentation:**
  - `README.md` - Project overview
  - `IMPLEMENTATION_GUIDE.md` - Integration details
  - `QUICKSTART.md` - This file

---

## Summary

âœ… Project fully set up
âœ… Model implemented (~3M parameters)
âœ… Test script ready
âœ… LQR dataset generator ready
âœ… Documentation complete
âœ… Environment configured

**You're ready to start experimenting!**

Run `python test_model.py` to verify everything works, then start comparing with your LLM baseline.

Good luck! ðŸš€
