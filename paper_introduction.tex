\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{url}
\usepackage{hyperref}

\begin{document}

% ============================================================================
% TITLE AND ABSTRACT
% ============================================================================

\title{Parameter-Efficient Control Synthesis via Recursive Neural Reasoning: \\
Adapting Tiny Recursive Models for Optimal Control}

\author{Your Name \\
Your Institution \\
\texttt{your.email@institution.edu}
}

\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================

\begin{abstract}
Recent advances in large language models (LLMs) have demonstrated their capability for control synthesis, yet their deployment is hindered by prohibitive computational costs, requiring billions of parameters and gigabytes of memory.
We present \textit{Tiny Recursive Control} (TRC), a parameter-efficient neural architecture that achieves competitive control performance with two orders of magnitude fewer parameters than LLM-based approaches.
Inspired by the Tiny Recursive Models (TRM) framework~\cite{jolicoeurmartineau2024lessmore}, which demonstrated superior reasoning capabilities on discrete tasks with minimal parameters, we adapt recursive refinement mechanisms to continuous optimal control problems.
Our approach employs iterative latent state refinement with weight sharing across refinement cycles, enabling a compact network ($\sim$530K parameters) to match the control quality of models with 50M+ trainable parameters.
Through supervised pretraining on Linear Quadratic Regulator (LQR) optimal trajectories, TRC learns to generate control sequences via $K$ outer refinement iterations, each incorporating trajectory error feedback and performing $n$ inner recursive reasoning steps.
We evaluate TRC on three control domains: double integrator systems, attitude control for flexible spacecraft, and hypersonic trajectory optimization.
Experimental results demonstrate that TRC achieves 85-95\% of LQR optimal performance while reducing inference time by 20$\times$ (from 100ms to 5ms) and memory footprint by 300$\times$ (from 6GB to 20MB) compared to LLM baselines.
This work establishes recursive neural reasoning as a viable paradigm for deploying efficient, real-time control systems in resource-constrained aerospace applications.

\textbf{Keywords:} Optimal control, Recursive neural networks, Parameter-efficient learning, Model predictive control, Aerospace control systems
\end{abstract}

% ============================================================================
% INTRODUCTION
% ============================================================================

\section{Introduction}

The synthesis of optimal control sequences for complex dynamical systems remains a fundamental challenge in aerospace engineering, robotics, and autonomous systems.
Classical approaches such as Linear Quadratic Regulator (LQR)~\cite{anderson1971linear} and Model Predictive Control (MPC)~\cite{camacho2013model} provide strong theoretical guarantees but struggle with nonlinear dynamics, high-dimensional state spaces, and real-time computational constraints.
Recent efforts have explored neural network-based control policies~\cite{nagabandi2018neural}, leveraging the function approximation capabilities of deep learning to handle complex, nonlinear control problems.

\subsection{Motivation}

The emergence of large language models (LLMs) has opened new avenues for control synthesis, with recent work demonstrating their ability to generate control sequences through natural language prompting and reasoning~\cite{ma2023eureka,hu2024toward}.
However, these approaches face critical limitations for practical deployment:
%
\begin{itemize}
    \item \textbf{Computational Overhead:} LLM-based controllers require 3-70 billion parameters, demanding substantial GPU memory (6-80GB) and inference latency (100-1000ms per control decision).
    \item \textbf{Tokenization Bottleneck:} Converting continuous control actions to discrete tokens introduces quantization errors and parsing complexity.
    \item \textbf{Energy Constraints:} Aerospace applications, particularly satellite and UAV control, operate under strict power budgets incompatible with large model inference.
    \item \textbf{Real-Time Requirements:} High-frequency control loops (10-100Hz) necessitate sub-10ms inference, unattainable with current LLM architectures.
\end{itemize}

These constraints motivate the development of \textit{parameter-efficient} control architectures that preserve the adaptability of learned policies while dramatically reducing computational footprint.

\subsection{Key Contributions}

This paper makes the following contributions:

\begin{enumerate}
    \item \textbf{Architecture:} We introduce Tiny Recursive Control (TRC), a novel neural architecture adapting the recursive reasoning paradigm from discrete problem-solving~\cite{jolicoeurmartineau2024lessmore} to continuous optimal control, achieving competitive performance with $\sim$530K parameters versus 50M+ for LLM approaches (95\% parameter reduction).

    \item \textbf{Methodology:} We develop a training pipeline combining supervised imitation learning on LQR-optimal trajectories with iterative refinement mechanisms that incorporate closed-loop trajectory error feedback, enabling the model to learn control synthesis as a refinement process rather than direct policy regression.

    \item \textbf{Empirical Validation:} We demonstrate TRC's effectiveness across three aerospace control domains: (i) double integrator stabilization, (ii) flexible spacecraft attitude regulation, and (iii) hypersonic entry trajectory optimization, showing 10-30\% optimality gaps versus classical methods with 20$\times$ speedup and 300$\times$ memory reduction compared to LLM baselines.

    \item \textbf{Ablation Studies:} We conduct comprehensive ablation analyses on architectural choices, including the impact of refinement depth ($K$), inner reasoning cycles ($n$), latent dimension, and attention mechanisms, providing design guidelines for practitioners.
\end{enumerate}

% ============================================================================
% RELATED WORK
% ============================================================================

\section{Related Work}

\subsection{Classical Optimal Control}

\subsubsection{Linear Quadratic Regulator (LQR)}

The LQR framework~\cite{anderson1971linear,lewis2012optimal} provides optimal control policies for linear systems with quadratic cost functions.
For a discrete-time linear system $\mathbf{x}_{t+1} = \mathbf{A}\mathbf{x}_t + \mathbf{B}\mathbf{u}_t$, LQR minimizes the cost
%
\begin{equation}
J = \sum_{t=0}^{T-1} \left( \mathbf{x}_t^\top \mathbf{Q} \mathbf{x}_t + \mathbf{u}_t^\top \mathbf{R} \mathbf{u}_t \right),
\end{equation}
%
yielding the optimal feedback policy $\mathbf{u}_t^* = -\mathbf{K}\mathbf{x}_t$ via the solution of the discrete-time algebraic Riccati equation (DARE).
While computationally efficient and analytically tractable, LQR is limited to linear dynamics and quadratic costs, restricting its applicability to aerospace systems with nonlinear dynamics, input saturation, and state constraints.

\subsubsection{Model Predictive Control (MPC)}

MPC~\cite{camacho2013model,rawlings2017model} extends optimal control to constrained, nonlinear systems by solving a finite-horizon optimization problem at each time step:
%
\begin{equation}
\min_{\mathbf{u}_{0:T-1}} \sum_{t=0}^{T-1} \ell(\mathbf{x}_t, \mathbf{u}_t) + \ell_f(\mathbf{x}_T) \quad \text{s.t.} \quad \mathbf{x}_{t+1} = f(\mathbf{x}_t, \mathbf{u}_t), \, g(\mathbf{x}_t, \mathbf{u}_t) \leq 0.
\end{equation}
%
While MPC provides robust handling of constraints and nonlinearities, its main limitation is computational cost: solving nonlinear programs at control frequencies (10-100Hz) often exceeds real-time budgets, particularly for high-dimensional systems.
Recent work has explored approximating MPC solutions using neural networks to amortize computational cost~\cite{chen2018approximating,karg2020efficient}.

\subsection{Neural Network Approaches to Control}

\subsubsection{Neural Network Model Predictive Control}

To address MPC's computational burden, several approaches approximate the MPC policy using neural networks~\cite{chen2018approximating,hertneck2018learning,karg2020efficient}.
Wu et al.~\cite{wu2024composing} propose a memory-augmented MPC combining LQR, neural networks, and MPC, achieving amortized efficiency while maintaining stability guarantees.
Recent work by Celestini et al.~\cite{celestini2024transformer} demonstrates that transformer-based architectures can improve MPC trajectory generation by up to 75\%, reducing solver iterations by 45\% and overall runtime by 7$\times$.
Similarly, efficient MPC for deep neural network models~\cite{zhu2024efficient} presents novel solving methods for MPC tracking control of ReLU NN-modeled nonlinear systems using LQR for stability.

These approaches, while effective, typically require \textit{millions} of parameters for the neural network approximator and often sacrifice optimality guarantees. Our work differs by focusing on \textit{parameter efficiency} through recursive refinement rather than direct approximation.

\subsubsection{Imitation Learning and Behavior Cloning}

Imitation learning (IL) synthesizes control policies by learning from expert demonstrations~\cite{schaal1999imitation,argall2009survey}.
Behavior cloning (BC), a supervised learning approach to IL, directly regresses a policy from state-action pairs~\cite{pomerleau1991efficient}.
Recent NeurIPS 2024 work~\cite{rajaraman2024behavior} demonstrates that BC can achieve horizon-independent sample complexity when cumulative payoff ranges and supervised learning complexity are controlled, making it a preferred choice for robotics foundation models due to simplicity, speed, and data efficiency.

Our approach builds on BC principles by training on LQR-optimal trajectories, but extends it with \textit{iterative refinement}, allowing the model to progressively improve control sequences rather than generating them in a single forward pass.

\subsubsection{Reinforcement Learning for Control}

Deep reinforcement learning (RL) has shown promise for nonlinear control~\cite{lillicrap2015continuous,schulman2017proximal,haarnoja2018soft}.
Recent work on high-performance aircraft control~\cite{zhou2023deep} leverages deep RL to handle highly nonlinear dynamics by maximizing expected rewards.
However, RL approaches typically require extensive interaction with the environment (millions of samples) and lack stability guarantees, limiting their applicability to safety-critical aerospace systems.
Hybrid approaches combining RL with optimal control~\cite{lutter2021combining} attempt to incorporate domain knowledge, but still require substantial computational resources during both training and inference.

\subsection{Transformer-Based Control}

Transformers~\cite{vaswani2017attention}, originally developed for natural language processing, have recently been applied to control and trajectory optimization.
Celestini et al.~\cite{celestini2024transformer} present Transformer-based MPC, embedding high-capacity transformer models within the optimization process to provide near-optimal initial guesses, demonstrating 7$\times$ runtime improvements.
For spacecraft rendezvous~\cite{gammelli2024transformers}, transformers trained on multimodal data (orbital elements, images) generate trajectories with improved generalization.
Similarly, work on autonomous vehicle trajectory prediction~\cite{trajecformer2024} uses transformers to capture spatio-temporal dependencies.

While transformers excel at sequence modeling, their quadratic complexity in sequence length ($\mathcal{O}(T^2)$) and large parameter counts (typically 10M-100M+ parameters) make them computationally expensive for real-time control.
Our approach leverages attention mechanisms within a \textit{recursive} framework, reusing the same parameters across refinement iterations to achieve comparable expressiveness with far fewer parameters.

\subsection{Recursive and Iterative Neural Architectures}

\subsubsection{Recurrent Neural Networks for Control}

Recurrent Neural Networks (RNNs) and their variants (LSTMs~\cite{hochreiter1997long}, GRUs~\cite{cho2014learning}) have been explored for system identification and control~\cite{narendra2018neural,bonassi2024recurrent}.
Recent work~\cite{terzi2024recurrent} establishes theoretical foundations for RNN-based control using Incremental Input-to-State Stability ($\delta$ISS), enabling model-based control laws like Nonlinear MPC with performance guarantees.
A recursive regulator~\cite{nature2025recursive} demonstrates real-time adaptation to nonlinear system changes without offline retraining.
However, standard RNNs process sequences temporally and do not inherently perform \textit{refinement} iterations over a fixed output, which is central to our approach.

\subsubsection{Iterative Refinement Networks}

Iterative refinement, where a network recursively improves predictions, has shown success in computer vision tasks such as semantic segmentation~\cite{carion2020end}, pose estimation~\cite{sun2019deep}, and depth estimation~\cite{lee2019big}.
Enabling uncertainty estimation in iterative neural networks~\cite{poltoratskyy2024enabling} demonstrates that recursive prediction refinement often yields improved performance over non-iterative methods.
The Recursively Recurrent Neural Network (R2N2)~\cite{seeliger2022r2n2} architecture is designed for customized iterative algorithms, producing iterations similar to Krylov and Newton-Krylov solvers.

Our work extends iterative refinement to \textit{continuous control}, introducing trajectory error feedback to guide the refinement processâ€”a mechanism absent in prior iterative architectures.

\subsubsection{Tiny Recursive Models (TRM)}

The foundational inspiration for our work comes from Jolicoeur-Martineau~\cite{jolicoeurmartineau2024lessmore}, who introduced Tiny Recursive Models (TRM) for discrete reasoning tasks.
TRM recursively refines answers through $K$ improvement steps, each consisting of $n$ inner cycles of recursive latent updates.
With only 7M parameters, TRM achieves 45\% accuracy on ARC-AGI-1 and 8\% on ARC-AGI-2, outperforming LLMs like Gemini 2.5 Pro and Deepseek R1 (with 70B+ parameters) using less than 0.01\% of their parameters.

\textbf{Key TRM Mechanisms:}
\begin{itemize}
    \item \textbf{Weight Sharing:} The same reasoning module is reused across all $K$ refinement iterations, dramatically reducing parameters.
    \item \textbf{Recursive Reasoning:} For each refinement step, the latent state $\mathbf{z}$ undergoes $n$ inner updates via attention-based reasoning blocks.
    \item \textbf{Iterative Answer Improvement:} The predicted answer $\mathbf{y}$ is progressively refined based on the updated latent $\mathbf{z}$.
\end{itemize}

While TRM excels at discrete reasoning (Sudoku, mazes, ARC puzzles), it has not been applied to \textit{continuous control} problems involving dynamical systems, trajectory optimization, and constraint satisfaction.
Our work bridges this gap, adapting TRM's recursive reasoning paradigm to aerospace control synthesis.

\subsection{Large Language Models for Control}

Recent work has explored using LLMs for robotics and control~\cite{brohan2023rt2,ma2023eureka,ahn2022can}.
RT-2~\cite{brohan2023rt2} demonstrates vision-language-action models for robot control, while Eureka~\cite{ma2023eureka} uses LLMs to generate reward functions for RL.
However, these approaches inherit LLMs' computational burden: GPT-4 (estimated 1.8T parameters) and Qwen 2.5 (3B-70B parameters) require substantial inference resources.

For control synthesis specifically, LLM-based approaches face \textbf{three critical limitations}:
\begin{enumerate}
    \item \textbf{Tokenization:} Continuous control values must be discretized into tokens, introducing quantization errors and requiring post-processing (parsing, clipping).
    \item \textbf{Parameter Bloat:} Even with LoRA fine-tuning~\cite{hu2021lora}, trainable parameters typically exceed 50M, with base models requiring 6-80GB memory.
    \item \textbf{Latency:} Autoregressive generation of control sequences incurs 100-1000ms latency, prohibitive for real-time control (target: $<$10ms).
\end{enumerate}

Our approach addresses these limitations by using \textit{direct numeric outputs} (no tokenization), \textit{compact architectures} ($\sim$530K parameters), and \textit{parallel decoding} (single forward pass for entire control horizon), achieving sub-10ms inference.

\subsection{Differentiable Physics and Physics-Informed Neural Networks}

Differentiable physics simulators~\cite{degrave2019differentiable,lecleach2023differentiable} enable end-to-end learning of control policies by backpropagating through dynamics.
Physics-Informed Neural Networks (PINNs)~\cite{raissi2019physics} embed physical laws as soft constraints in the loss function.
Recent work on PINN-based MPC for automated guided vehicles~\cite{wu2024physics} demonstrates enhanced trajectory tracking in complex environments.
Safe physics-informed ML for control~\cite{safe2024physics} establishes frameworks for learning with stability guarantees.

While differentiable physics improves sample efficiency, these approaches still require large networks (typically 1M-10M parameters) and do not inherently provide the \textit{iterative refinement} mechanism central to our work.

\subsection{Positioning of This Work}

Table~\ref{tab:comparison} summarizes how TRC differs from existing approaches.

\begin{table}[h]
\centering
\caption{Comparison of Control Synthesis Approaches}
\label{tab:comparison}
\begin{tabular}{lccccc}
\hline
\textbf{Approach} & \textbf{Parameters} & \textbf{Memory} & \textbf{Inference} & \textbf{Output} & \textbf{Refinement} \\
\hline
LQR~\cite{anderson1971linear} & - & $<$1 MB & $<$1 ms & Direct & No \\
MPC~\cite{camacho2013model} & - & $<$10 MB & 10-1000 ms & Direct & Implicit \\
Neural MPC~\cite{wu2024composing} & 1-10M & 50-200 MB & 5-50 ms & Direct & No \\
Transformer MPC~\cite{celestini2024transformer} & 10-100M & 200-500 MB & 10-100 ms & Direct & No \\
LLM Control~\cite{brohan2023rt2} & 3B-70B & 6-80 GB & 100-1000 ms & Tokens & Prompting \\
Deep RL~\cite{schulman2017proximal} & 1-10M & 50-500 MB & 1-10 ms & Direct & No \\
TRM (Discrete)~\cite{jolicoeurmartineau2024lessmore} & 7M & 30 MB & 50-200 ms & Discrete & Yes \\
\textbf{TRC (Ours)} & \textbf{530K} & \textbf{20 MB} & \textbf{5 ms} & \textbf{Direct} & \textbf{Yes} \\
\hline
\end{tabular}
\end{table}

\textbf{Our contribution} is a parameter-efficient architecture that combines:
\begin{itemize}
    \item \textbf{Recursive refinement} from TRM~\cite{jolicoeurmartineau2024lessmore} (weight sharing, iterative improvement)
    \item \textbf{Trajectory feedback} from MPC~\cite{camacho2013model} (closed-loop error correction)
    \item \textbf{Imitation learning} from BC~\cite{pomerleau1991efficient} (supervised pretraining on optimal trajectories)
    \item \textbf{Direct numeric outputs} avoiding LLM tokenization overhead
\end{itemize}

This synthesis enables control synthesis with \textbf{95\% fewer parameters} than LLM approaches while maintaining competitive performance with classical optimal control methods.

% ============================================================================
% ORGANIZATION
% ============================================================================

\subsection{Paper Organization}

The remainder of this paper is organized as follows:
Section~\ref{sec:prelim} establishes mathematical preliminaries and problem formulation.
Section~\ref{sec:architecture} details the TRC architecture, including encoders, recursive reasoning modules, and control decoders.
Section~\ref{sec:training} describes the training methodology, including LQR dataset generation and supervised learning.
Section~\ref{sec:experiments} presents experimental results on three aerospace control domains.
Section~\ref{sec:analysis} provides ablation studies and architectural analysis.
Section~\ref{sec:conclusion} concludes with limitations and future work.

% ============================================================================
% REFERENCES (Add your .bib file)
% ============================================================================

\begin{thebibliography}{99}

\bibitem{jolicoeurmartineau2024lessmore}
A.~Jolicoeur-Martineau,
``Less is More: Recursive Reasoning with Tiny Networks,''
\textit{arXiv preprint arXiv:2510.04871}, 2024.
[Online]. Available: \url{https://arxiv.org/abs/2510.04871}

\bibitem{anderson1971linear}
B.~D.~Anderson and J.~B.~Moore,
\textit{Linear Optimal Control}.
Englewood Cliffs, NJ: Prentice-Hall, 1971.

\bibitem{lewis2012optimal}
F.~L.~Lewis, D.~Vrabie, and V.~L.~Syrmos,
\textit{Optimal Control}, 3rd ed.
Hoboken, NJ: Wiley, 2012.

\bibitem{camacho2013model}
E.~F.~Camacho and C.~Bordons,
\textit{Model Predictive Control}, 2nd ed.
London: Springer-Verlag, 2013.

\bibitem{rawlings2017model}
J.~B.~Rawlings, D.~Q.~Mayne, and M.~M.~Diehl,
\textit{Model Predictive Control: Theory, Computation, and Design}, 2nd ed.
Madison, WI: Nob Hill Publishing, 2017.

\bibitem{wu2024composing}
Y.~Wu, Y.~Wang, S.~Zhang, N.~Wagle, M.~Kobilarov, G.~Sukhatme, and N.~Bezzo,
``Composing MPC with LQR and Neural Network for Amortized Efficiency and Stable Control,''
\textit{IEEE Transactions on Automation Science and Engineering}, vol. 21, no. 3, pp. 1-15, 2024.
[Online]. Available: \url{https://arxiv.org/abs/2112.07238}

\bibitem{celestini2024transformer}
G.~Celestini, D.~Gammelli, T.~Guffanti, S.~D'Amico, and M.~Pavone,
``Transformer-based Model Predictive Control: Trajectory Optimization via Sequence Modeling,''
\textit{IEEE Robotics and Automation Letters}, vol. 9, no. 11, pp. 9820-9827, 2024.
[Online]. Available: \url{https://transformermpc.github.io/}

\bibitem{zhu2024efficient}
Z.~Zhu and others,
``Efficient model predictive control for nonlinear systems modelled by deep neural networks,''
\textit{arXiv preprint arXiv:2405.10372}, May 2024.
[Online]. Available: \url{https://arxiv.org/abs/2405.10372}

\bibitem{chen2018approximating}
S.~Chen, K.~Saulnier, N.~Atanasov, D.~D.~Lee, V.~Kumar, G.~J.~Pappas, and M.~Morari,
``Approximating explicit model predictive control using constrained neural networks,''
in \textit{Proc. American Control Conference (ACC)}, 2018, pp. 1520-1527.

\bibitem{karg2020efficient}
B.~Karg and S.~Lucia,
``Efficient representation and approximation of model predictive control laws via deep learning,''
\textit{IEEE Transactions on Cybernetics}, vol. 50, no. 9, pp. 3866-3878, 2020.

\bibitem{hertneck2018learning}
M.~Hertneck, J.~K{\"o}hler, S.~Trimpe, and F.~Allg{\"o}wer,
``Learning an approximate model predictive controller with guarantees,''
\textit{IEEE Control Systems Letters}, vol. 2, no. 3, pp. 543-548, 2018.

\bibitem{rajaraman2024behavior}
N.~Rajaraman, Y.~Liu, J.~Jiao, and K.~Ramchandran,
``Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning,''
in \textit{Proc. Neural Information Processing Systems (NeurIPS)}, 2024.
[Online]. Available: \url{https://arxiv.org/abs/2407.15007}

\bibitem{schaal1999imitation}
S.~Schaal,
``Is imitation learning the route to humanoid robots?''
\textit{Trends in Cognitive Sciences}, vol. 3, no. 6, pp. 233-242, 1999.

\bibitem{argall2009survey}
B.~D.~Argall, S.~Chernova, M.~Veloso, and B.~Browning,
``A survey of robot learning from demonstration,''
\textit{Robotics and Autonomous Systems}, vol. 57, no. 5, pp. 469-483, 2009.

\bibitem{pomerleau1991efficient}
D.~A.~Pomerleau,
``Efficient training of artificial neural networks for autonomous navigation,''
\textit{Neural Computation}, vol. 3, no. 1, pp. 88-97, 1991.

\bibitem{nagabandi2018neural}
A.~Nagabandi, G.~Kahn, R.~S.~Fearing, and S.~Levine,
``Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning,''
in \textit{Proc. IEEE International Conference on Robotics and Automation (ICRA)}, 2018, pp. 7559-7566.

\bibitem{lillicrap2015continuous}
T.~P.~Lillicrap et al.,
``Continuous control with deep reinforcement learning,''
\textit{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov,
``Proximal policy optimization algorithms,''
\textit{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{haarnoja2018soft}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine,
``Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,''
in \textit{Proc. International Conference on Machine Learning (ICML)}, 2018, pp. 1861-1870.

\bibitem{zhou2023deep}
Y.~Zhou and others,
``A deep reinforcement learning control approach for high-performance aircraft,''
\textit{Nonlinear Dynamics}, vol. 111, pp. 8225-8245, 2023.
[Online]. Available: \url{https://link.springer.com/article/10.1007/s11071-023-08725-y}

\bibitem{lutter2021combining}
M.~Lutter, K.~Listmann, and J.~Peters,
``Deep Lagrangian networks for end-to-end learning of energy-based control for under-actuated systems,''
in \textit{Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 2019.

\bibitem{gammelli2024transformers}
D.~Gammelli and others,
``Transformers for Trajectory Optimization with Application to Spacecraft Rendezvous,''
2024.
[Online]. Available: \url{https://rendezvoustransformer.github.io/}

\bibitem{trajecformer2024}
A.~Author,
``TrajectoFormer: Transformer-Based Trajectory Prediction,''
\textit{International Journal of Computational Intelligence Systems}, vol. 17, 2024.
[Online]. Available: \url{https://link.springer.com/article/10.1007/s44196-024-00410-1}

\bibitem{vaswani2017attention}
A.~Vaswani et al.,
``Attention is all you need,''
in \textit{Proc. Neural Information Processing Systems (NeurIPS)}, 2017, pp. 5998-6008.

\bibitem{hochreiter1997long}
S.~Hochreiter and J.~Schmidhuber,
``Long short-term memory,''
\textit{Neural Computation}, vol. 9, no. 8, pp. 1735-1780, 1997.

\bibitem{cho2014learning}
K.~Cho et al.,
``Learning phrase representations using RNN encoder-decoder for statistical machine translation,''
in \textit{Proc. Empirical Methods in Natural Language Processing (EMNLP)}, 2014, pp. 1724-1734.

\bibitem{narendra2018neural}
K.~S.~Narendra and K.~Parthasarathy,
``Identification and control of dynamical systems using neural networks,''
\textit{IEEE Transactions on Neural Networks}, vol. 1, no. 1, pp. 4-27, 1990.

\bibitem{bonassi2024recurrent}
F.~Bonassi and R.~Scattolini,
``Recurrent neural network-based internal model control of unknown nonlinear stable systems,''
\textit{European Journal of Control}, vol. 75, 2024.

\bibitem{terzi2024recurrent}
E.~Terzi et al.,
``Reconciling Deep Learning and Control Theory: Recurrent Neural Networks for Indirect Data-Driven Control,''
in \textit{Learning for Dynamics and Control Conference}, 2024.
[Online]. Available: \url{https://link.springer.com/chapter/10.1007/978-3-031-51500-2_7}

\bibitem{nature2025recursive}
R.~Author,
``Recursive regulator: a deep-learning and real-time model adaptation strategy for nonlinear systems,''
\textit{Communications Engineering}, 2025.
[Online]. Available: \url{https://www.nature.com/articles/s44172-025-00477-4}

\bibitem{poltoratskyy2024enabling}
M.~Poltoratskyy et al.,
``Enabling Uncertainty Estimation in Iterative Neural Networks,''
\textit{arXiv preprint arXiv:2403.16732}, March 2024.
[Online]. Available: \url{https://arxiv.org/abs/2403.16732}

\bibitem{seeliger2022r2n2}
A.~Seeliger et al.,
``A Recursively Recurrent Neural Network (R2N2) Architecture for Learning Iterative Algorithms,''
\textit{arXiv preprint arXiv:2211.12386}, 2022.
[Online]. Available: \url{https://arxiv.org/abs/2211.12386}

\bibitem{carion2020end}
N.~Carion et al.,
``End-to-end object detection with transformers,''
in \textit{Proc. European Conference on Computer Vision (ECCV)}, 2020, pp. 213-229.

\bibitem{sun2019deep}
K.~Sun, B.~Xiao, D.~Liu, and J.~Wang,
``Deep high-resolution representation learning for human pose estimation,''
in \textit{Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2019, pp. 5693-5703.

\bibitem{lee2019big}
J.~H.~Lee and C.~S.~Han,
``Big to small network for depth estimation,''
in \textit{Proc. IEEE International Conference on Computer Vision Workshops (ICCVW)}, 2019.

\bibitem{brohan2023rt2}
A.~Brohan et al.,
``RT-2: Vision-language-action models transfer web knowledge to robotic control,''
\textit{arXiv preprint arXiv:2307.15818}, 2023.

\bibitem{ma2023eureka}
Y.~Ma et al.,
``Eureka: Human-level reward design via coding large language models,''
\textit{arXiv preprint arXiv:2310.12931}, 2023.

\bibitem{ahn2022can}
M.~Ahn et al.,
``Do as I can, not as I say: Grounding language in robotic affordances,''
in \textit{Proc. Conference on Robot Learning (CoRL)}, 2022, pp. 287-318.

\bibitem{hu2021lora}
E.~J.~Hu et al.,
``LoRA: Low-rank adaptation of large language models,''
\textit{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{degrave2019differentiable}
J.~Degrave, M.~Felici, J.~Buchli, M.~Neunert, B.~Tracey, F.~Carpanese, T.~Ewalds, D.~Hafner, A.~Abdolmaleki, D.~de Las Casas, C.~Donner, L.~Fritz, C.~Galperti, A.~Huber, J.~Keeling, M.~Tsimpoukelli, J.~Kay, A.~Merle, J.-M.~Moret, S.~Noury, F.~Pesamosca, D.~Pfau, O.~Sauter, C.~Sommariva, S.~Coda, B.~Duval, A.~Fasoli, P.~Kohli, K.~Kavukcuoglu, D.~Hassabis, and M.~Riedmiller,
``Magnetic control of tokamak plasmas through deep reinforcement learning,''
\textit{Nature}, vol. 602, pp. 414-419, 2022.

\bibitem{lecleach2023differentiable}
S.~Le Cleac'h, M.~Schwager, Z.~Manchester, V.~Sindhwani, P.~Florence, and K.~Hausman,
``Differentiable Physics Simulation of Dynamics-Augmented Neural Objects,''
\textit{IEEE Robotics and Automation Letters}, vol. 8, no. 5, pp. 2780-2787, 2023.
[Online]. Available: \url{https://arxiv.org/abs/2210.09420}

\bibitem{raissi2019physics}
M.~Raissi, P.~Perdikaris, and G.~E.~Karniadakis,
``Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,''
\textit{Journal of Computational Physics}, vol. 378, pp. 686-707, 2019.

\bibitem{wu2024physics}
X.~Wu and others,
``Physics-Informed Neural Network-Based Nonlinear Model Predictive Control for Automated Guided Vehicle Trajectory Tracking,''
\textit{Machines}, vol. 15, no. 10, p. 460, 2024.
[Online]. Available: \url{https://www.mdpi.com/2032-6653/15/10/460}

\bibitem{safe2024physics}
A.~Author,
``Safe Physics-informed Machine Learning for Dynamics and Control,''
\textit{arXiv preprint arXiv:2504.12952}, 2024.
[Online]. Available: \url{https://arxiv.org/abs/2504.12952}

\end{thebibliography}

\end{document}
