# TRM vs TRC: Implementation Gap Analysis

**Generated:** 2025-10-21
**Purpose:** Identify specific architectural differences between actual TRM implementation and current TRC two-level mode

---

## Executive Summary

### Current Status
- ‚úÖ **TRC Two-Level Mode**: ~85% faithful to TRM architecture
- ‚úÖ **Core Principles**: Fully implemented (weight sharing, hierarchy, recursion)
- ‚ö†Ô∏è **Component-Level Differences**: Several implementation details differ

### Recommendation
**Selective Enhancement**: Add specific TRM features that may improve control performance while keeping control-specific advantages.

---

## Detailed Component-by-Component Comparison

### 1. Activation Functions

| Component | TRM Implementation | TRC Implementation | Impact | Action |
|-----------|-------------------|-------------------|---------|--------|
| **FFN Activation** | SwiGLU | SiLU | Medium | Consider upgrading |
| **Position** | Inside RecursiveReasoningBlock | Inside RecursiveReasoningBlock | - | - |
| **Code Location** | `trm.py:84-87` | `recursive_reasoning.py:56-61` | - | - |

**SwiGLU Details (TRM):**
```python
# From TinyRecursiveModels/models/layers.py
class SwiGLU(nn.Module):
    def __init__(self, hidden_size, expansion=4.0):
        super().__init__()
        intermediate_size = int(hidden_size * expansion)
        self.fc_in = nn.Linear(hidden_size, 2 * intermediate_size, bias=False)
        self.fc_out = nn.Linear(intermediate_size, hidden_size, bias=False)

    def forward(self, x):
        gate, x = self.fc_in(x).chunk(2, dim=-1)
        x = x * F.silu(gate)  # Gated activation
        return self.fc_out(x)
```

**Current TRC (SiLU):**
```python
# src/models/recursive_reasoning.py:56-61
self.ffn = nn.Sequential(
    nn.Linear(latent_dim, hidden_dim),
    nn.SiLU(),  # Simple SiLU
    nn.Dropout(dropout),
    nn.Linear(hidden_dim, latent_dim),
)
```

**Difference:** SwiGLU uses gated activation (more parameters, more expressive) vs plain SiLU
**Benefit of SwiGLU:** Better gradient flow, more capacity for same layer depth
**Parameter Impact:** +50% more parameters in FFN

---

### 2. Normalization

| Component | TRM Implementation | TRC Implementation | Impact | Action |
|-----------|-------------------|-------------------|---------|--------|
| **Norm Type** | RMS Norm | LayerNorm | Low-Medium | Consider upgrading |
| **Norm Position** | Post-norm | Pre-norm | Medium | Consider upgrading |
| **Code Location** | `trm.py:96,100,103` | `recursive_reasoning.py:53,62,92,96` | - | - |

**RMS Norm Details (TRM):**
```python
# From TinyRecursiveModels/models/layers.py
def rms_norm(hidden_states, variance_epsilon=1e-5):
    """Root Mean Square Layer Normalization (simpler than LayerNorm)"""
    input_dtype = hidden_states.dtype
    hidden_states = hidden_states.to(torch.float32)
    variance = hidden_states.pow(2).mean(-1, keepdim=True)
    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)
    return hidden_states.to(input_dtype)
```

**Current TRC (LayerNorm):**
```python
# src/models/recursive_reasoning.py:53,62
self.norm1 = nn.LayerNorm(latent_dim)  # Includes learnable affine params
self.norm2 = nn.LayerNorm(latent_dim)
```

**Difference:** RMS norm has no learnable parameters (affine), no mean centering
**Benefit of RMS:** Slightly faster, fewer parameters, empirically works well
**Benefit of LayerNorm:** More stable in some cases, learnable scale/shift

**Post-Norm vs Pre-Norm:**
```python
# TRM (Post-norm):
hidden_states = rms_norm(hidden_states + self.self_attn(...))
hidden_states = rms_norm(hidden_states + self.mlp(...))

# TRC (Pre-norm):
z = self.norm1(z + attn_out)
z = self.norm2(z + ffn_out)
```

**Difference:** Post-norm applies normalization AFTER residual addition
**Post-norm Benefit:** Better gradient scaling, used in TRM paper

---

### 3. Architecture Pattern

| Component | TRM Implementation | TRC Implementation | Match? | Action |
|-----------|-------------------|-------------------|---------|--------|
| **Residual + Norm Order** | hidden + layer ‚Üí norm | hidden + layer ‚Üí norm | ‚úÖ Same | None |
| **Context Injection** | At module start | At module start & block level | ‚ö†Ô∏è Different | Review |

**TRM Pattern:**
```python
# trm.py:111-115 (ReasoningModule)
def forward(self, hidden_states, input_injection, **kwargs):
    hidden_states = hidden_states + input_injection  # Inject once at start
    for layer in self.layers:
        hidden_states = layer(hidden_states=hidden_states, **kwargs)
    return hidden_states
```

**TRC Pattern:**
```python
# recursive_reasoning.py:299-306 (ControlReasoningModule)
def forward(self, hidden_state, input_injection):
    z = hidden_state + input_injection  # Inject at module start
    for layer in self.layers:
        z = layer(z, context=None)  # No further injection
    return z

# But RecursiveReasoningBlock also accepts context:
def forward(self, z, context):
    if context is not None:
        z = z + context  # Can inject at block level too
    # ... rest of block
```

**Analysis:** TRC has flexibility for both module-level and block-level injection, TRM only uses module-level
**Recommendation:** Keep TRC's flexibility

---

### 4. Two-Level Hierarchy Implementation

| Component | TRM Implementation | TRC Implementation | Match? | Recommendation |
|-----------|-------------------|-------------------|---------|----------------|
| **z_H, z_L states** | ‚úÖ Separate latents | ‚úÖ Separate latents | ‚úÖ Exact | Keep |
| **H_init, L_init** | nn.Buffer | nn.Parameter | ‚ö†Ô∏è Different | Review |
| **Shared L_level** | ‚úÖ Same module | ‚úÖ Same module | ‚úÖ Exact | Keep |
| **H_cycles** | ‚úÖ Outer loops | ‚úÖ Outer loops | ‚úÖ Exact | Keep |
| **L_cycles** | ‚úÖ Inner loops | ‚úÖ Inner loops | ‚úÖ Exact | Keep |
| **Gradient truncation** | ‚úÖ Optional | ‚úÖ Optional | ‚úÖ Exact | Keep |
| **Detached carry** | ‚úÖ z.detach() | ‚úÖ z.detach() | ‚úÖ Exact | Keep |

**H_init/L_init Difference:**

```python
# TRM (nn.Buffer - not updated by optimizer):
self.H_init = nn.Buffer(
    trunc_normal_init_(torch.empty(...), std=1.0),
    persistent=True
)
self.L_init = nn.Buffer(
    trunc_normal_init_(torch.empty(...), std=1.0),
    persistent=True
)

# TRC (nn.Parameter - updated by optimizer):
self.H_init = nn.Parameter(torch.randn(latent_dim) * 0.02)
self.L_init = nn.Parameter(torch.randn(latent_dim) * 0.02)
```

**Key Difference:**
- **TRM**: Initial states are FIXED (nn.Buffer), set once and never trained
- **TRC**: Initial states are LEARNABLE (nn.Parameter), optimized during training

**Implication:**
- TRM's approach: Initial states are task-agnostic "starting points"
- TRC's approach: Initial states learn to be good initializations for control problems
- **Recommendation**: TRC's learnable approach may be better for control domain

---

### 5. Input Injection Strategy

| Component | TRM Implementation | TRC Implementation | Match? |
|-----------|-------------------|-------------------|---------|
| **Low-level input** | `z_H + input_embeddings` | `z_H + z_initial + control_context` | ‚ö†Ô∏è Different |
| **High-level input** | `z_L` | `z_L` | ‚úÖ Same |

**TRM Low-Level Update:**
```python
# trm.py:211
for _L_step in range(L_cycles):
    z_L = self.L_level(z_L, z_H + input_embeddings)  # High-level guidance + problem
```

**TRC Low-Level Update:**
```python
# recursive_reasoning.py:438-440
for _ in range(self.L_cycles):
    low_level_input = z_H + z_initial + control_context  # More context!
    z_L = self.L_level(z_L, low_level_input)
```

**Analysis:**
- TRM: Injects high-level state + problem encoding
- TRC: Injects high-level state + problem encoding + control feedback + error feedback
- **Verdict**: TRC's richer injection is appropriate for control domain

---

### 6. Gradient Truncation

| Component | TRM Implementation | TRC Implementation | Match? |
|-----------|-------------------|-------------------|---------|
| **Implementation** | ‚úÖ torch.no_grad() for H_cycles-1 | ‚úÖ torch.no_grad() for H_cycles-1 | ‚úÖ Exact |
| **Detach logic** | ‚úÖ Detach after each iteration | ‚úÖ Detach after each iteration | ‚úÖ Exact |

**Both Implementations (Identical):**
```python
# TRM: trm.py:208-216
with torch.no_grad():
    for _H_step in range(self.config.H_cycles-1):
        for _L_step in range(self.config.L_cycles):
            z_L = self.L_level(z_L, z_H + input_embeddings)
        z_H = self.L_level(z_H, z_L)
# Last cycle WITH gradients
for _L_step in range(self.config.L_cycles):
    z_L = self.L_level(z_L, z_H + input_embeddings)
z_H = self.L_level(z_H, z_L)

# TRC: recursive_reasoning.py:430-444
if self.use_gradient_truncation and (H_step < self.H_cycles - 1):
    ctx = torch.no_grad()
else:
    ctx = torch.enable_grad()

with ctx:
    for _ in range(self.L_cycles):
        low_level_input = z_H + z_initial + control_context
        z_L = self.L_level(z_L, low_level_input)
    z_H = self.L_level(z_H, z_L)
```

**Verdict**: ‚úÖ Perfectly aligned on gradient truncation strategy

---

### 7. Features TRM Has That TRC Doesn't Use

#### 7.1 MLP-Transpose Option
**TRM Implementation:**
```python
# trm.py:70-76, 93-97
if config.mlp_t:
    self.mlp_t = SwiGLU(
        hidden_size=seq_len + puzzle_emb_len,  # Operates on L dimension!
        expansion=config.expansion,
    )
    # In forward:
    hidden_states = hidden_states.transpose(1,2)  # [B, D, L]
    out = self.mlp_t(hidden_states)
    hidden_states = rms_norm(hidden_states + out)
    hidden_states = hidden_states.transpose(1,2)  # [B, L, D]
```

**Purpose:** Mix information across sequence positions (alternative to attention)
**TRC Status:** ‚ùå Not implemented
**Relevance to Control:** ‚ö†Ô∏è Low (control doesn't use sequences the same way puzzles do)
**Recommendation:** Skip unless working with multi-step trajectory predictions

#### 7.2 Adaptive Computation Time (Full Q-Learning)
**TRM Implementation:**
```python
# trm.py:131-132, 221
self.q_head = CastedLinear(hidden_size, 2, bias=True)  # [halt, continue]

# Special initialization
with torch.no_grad():
    self.q_head.weight.zero_()
    self.q_head.bias.fill_(-5)

# Q-learning with exploration and bootstrapping (complex!)
```

**TRC Status:** ‚ö†Ô∏è Partially implemented but not used (recursive_reasoning.py:205-254)
**Recommendation:** Implement simplified version if needed, but fixed H_cycles works well

#### 7.3 RoPE (Rotary Position Embeddings)
**TRM Implementation:**
```python
# trm.py:140-143
if config.pos_encodings == "rope":
    self.rotary_emb = RotaryEmbedding(
        dim=config.hidden_size // config.num_heads,
        max_position_embeddings=config.seq_len + puzzle_emb_len,
        base=config.rope_theta
    )
```

**TRC Status:** ‚ùå Not implemented
**Relevance to Control:** ‚ùå Low (control uses single latent vectors, not sequences)
**Recommendation:** Skip

#### 7.4 Sparse Puzzle Embeddings
**TRM Implementation:**
```python
# trm.py:136-137
self.puzzle_emb = CastedSparseEmbedding(
    num_puzzle_identifiers, puzzle_emb_ndim, ...
)
```

**TRC Status:** ‚ùå Not implemented
**Relevance to Control:** ‚ö†Ô∏è Could be useful for multi-task learning (different systems)
**Recommendation:** Add if training on multiple control systems simultaneously

---

### 8. Features TRC Has That TRM Doesn't

#### 8.1 Control-Specific Embeddings
```python
# recursive_reasoning.py:370-380
self.control_embedding = nn.Linear(
    control_horizon * control_dim, latent_dim
)
self.error_embedding = nn.Linear(
    2,  # Final state error
    latent_dim
)
```

**Purpose:** Incorporate control feedback and trajectory errors
**Verdict:** ‚úÖ Essential for control domain

#### 8.2 Learnable Initial States
**Already covered above** - TRC uses nn.Parameter (learnable) vs TRM's nn.Buffer (fixed)

---

## Priority Recommendations

### High Priority (Likely to Help)

1. **Switch to SwiGLU Activation**
   - **Why:** More expressive, proven in TRM
   - **Impact:** Medium improvement potential
   - **Complexity:** Low (simple drop-in replacement)
   - **Code Change:** ~20 lines

2. **Switch to Post-Norm Architecture**
   - **Why:** TRM uses this, better gradient scaling
   - **Impact:** Medium improvement potential
   - **Complexity:** Low (reorder operations)
   - **Code Change:** ~10 lines

3. **Add RMS Normalization Option**
   - **Why:** Fewer parameters, faster, used in TRM
   - **Impact:** Low-Medium (mostly efficiency gain)
   - **Complexity:** Low (add option to config)
   - **Code Change:** ~30 lines

### Medium Priority (May Help)

4. **Experiment with Fixed vs Learnable Initial States**
   - **Why:** Test if TRM's approach (fixed) is better
   - **Impact:** Unknown (needs ablation study)
   - **Complexity:** Low (change nn.Parameter to nn.Buffer)
   - **Code Change:** ~5 lines

5. **Add Expansion Parameter to FFN**
   - **Why:** TRM uses 4√ó expansion, TRC uses 2√ó (hidden_dim=256 for latent=128)
   - **Impact:** Medium (more capacity)
   - **Complexity:** Low (config parameter)
   - **Code Change:** ~10 lines

### Low Priority (Nice to Have)

6. **Implement Simplified ACT**
   - **Why:** Adaptive halting could save computation
   - **Impact:** Low (fixed cycles work well)
   - **Complexity:** Medium (needs halting loss)
   - **Code Change:** ~100 lines

7. **Add Task Embeddings**
   - **Why:** Multi-task learning across control systems
   - **Impact:** Only if training on multiple systems
   - **Complexity:** Medium
   - **Code Change:** ~50 lines

---

## Recommended Enhancement Plan

### Phase 1: Core Architecture Alignment (1-2 days)

**Goal:** Bring TRC closer to TRM's core components

**Changes:**
1. Add SwiGLU activation as option
2. Add RMS normalization as option
3. Add post-norm option
4. Add expansion parameter to config
5. Add config flags to switch between modes

**Benefits:**
- Can A/B test TRM vs current features
- Minimal risk (all optional via config)
- Learn what matters for control domain

### Phase 2: Ablation Studies (2-3 days)

**Goal:** Understand which TRM features help control

**Experiments:**
1. Baseline: Current TRC two-level
2. +SwiGLU: Replace SiLU with SwiGLU
3. +RMS norm: Replace LayerNorm with RMS
4. +Post-norm: Reorder norm operations
5. Full TRM-style: All above combined
6. Fixed inits: nn.Buffer instead of nn.Parameter

**Metrics:**
- Final error (mean, std)
- Control cost
- Training time
- Inference speed
- Parameter count

### Phase 3: Best Configuration (1 day)

**Goal:** Select and document best architecture

**Deliverables:**
- Updated factory methods with best config
- Documentation of what works for control
- Comparison table: TRM features vs control performance

---

## Implementation Checklist

### Immediate Actions (Do These First)

- [ ] Create `SwiGLUFFN` class in `src/models/layers.py`
- [ ] Add `rms_norm` function to `src/models/layers.py`
- [ ] Update `RecursiveReasoningBlock` to support activation choice
- [ ] Update `RecursiveReasoningBlock` to support norm choice
- [ ] Update `RecursiveReasoningBlock` to support post-norm option
- [ ] Update `TRCConfig` with new options:
  - `activation_type: str = "silu"`  # or "swiglu"
  - `norm_type: str = "layernorm"`  # or "rmsnorm"
  - `norm_position: str = "pre"`  # or "post"
  - `ffn_expansion: float = 2.0`  # or 4.0 (TRM-style)
  - `learnable_inits: bool = True`  # or False (TRM-style fixed)

### Testing Actions

- [ ] Run `examples/test_two_level.py` with new options
- [ ] Add ablation test script
- [ ] Document results in `docs/AI/`

### Optional (Later)

- [ ] Implement simplified ACT
- [ ] Add multi-system task embeddings
- [ ] Port MLP-transpose for sequence tasks

---

## Summary Table

| Feature | TRM | TRC Current | Priority | Action |
|---------|-----|-------------|----------|--------|
| **Core Architecture** |
| Two-level hierarchy (z_H/z_L) | ‚úÖ | ‚úÖ | - | ‚úÖ Keep |
| Shared L_level module | ‚úÖ | ‚úÖ | - | ‚úÖ Keep |
| H_cycles / L_cycles | ‚úÖ | ‚úÖ | - | ‚úÖ Keep |
| Gradient truncation | ‚úÖ | ‚úÖ | - | ‚úÖ Keep |
| Detached carry | ‚úÖ | ‚úÖ | - | ‚úÖ Keep |
| **Components** |
| SwiGLU activation | ‚úÖ | ‚ùå (SiLU) | HIGH | üîß Add option |
| RMS normalization | ‚úÖ | ‚ùå (LayerNorm) | MEDIUM | üîß Add option |
| Post-norm architecture | ‚úÖ | ‚ùå (Pre-norm) | HIGH | üîß Add option |
| FFN expansion (4√ó) | ‚úÖ | ‚ö†Ô∏è (~2√ó) | MEDIUM | üîß Add config |
| Learnable inits | ‚ùå (fixed) | ‚úÖ | MEDIUM | üî¨ Ablate |
| **Advanced Features** |
| Full Q-learning ACT | ‚úÖ | ‚ùå | LOW | ‚è∏Ô∏è Skip for now |
| MLP-transpose | ‚úÖ | ‚ùå | LOW | ‚è∏Ô∏è Skip |
| RoPE embeddings | ‚úÖ | ‚ùå | LOW | ‚è∏Ô∏è Skip |
| Sparse task embeddings | ‚úÖ | ‚ùå | LOW | ‚è∏Ô∏è Skip |
| **Control-Specific** |
| Error feedback | ‚ùå | ‚úÖ | - | ‚úÖ Keep |
| Control embeddings | ‚ùå | ‚úÖ | - | ‚úÖ Keep |
| Residual decoder | ‚ùå | ‚úÖ | - | ‚úÖ Keep |

---

## Conclusion

**Current TRC two-level mode is already very good** (~85% TRM fidelity), but can be improved by:

1. ‚úÖ **Keep:** All hierarchical reasoning structure (perfect match)
2. üîß **Enhance:** Switch to TRM's components (SwiGLU, RMS, post-norm)
3. üî¨ **Test:** Ablation studies to see what helps control
4. ‚úÖ **Preserve:** Control-specific features (error feedback, residual updates)

**Next Steps:**
1. Implement SwiGLU, RMS norm, post-norm as config options
2. Run ablation studies
3. Document which TRM features help control tasks
4. Update factory methods with best configuration

This gives you the **best of both worlds**: TRM's proven architecture + control-specific innovations.
